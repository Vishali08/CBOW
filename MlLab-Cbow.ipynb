{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eea9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41162280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1757fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "path = \"C:/Users/visha/OneDrive/Desktop/bts.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data: str = file.read()\n",
    "\n",
    "data = re.sub(r'[,!?;-]', '.', data)\n",
    "data = word_tokenize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4b36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of tokens = 1686 \n",
      " ['BTS', '(', 'Korean', 'RR', ':', 'Bangtan', 'Sonyeondan', '.', 'lit', '.', 'Bulletproof', 'Boy', 'Scouts', ')', '.', 'also', 'known', 'as', 'the', 'Bangtan']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Number of tokens = {len(data)} \\n {data[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf0c17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  637\n",
      "Most frequent tokens:  [('.', 151), ('the', 76), ('and', 44), ('in', 43), ('[', 42), (']', 42), ('of', 36), ('to', 32), ('``', 24), (\"''\", 24), ('their', 20), ('on', 19), ('BTS', 18), ('The', 18), ('Korean', 16), ('(', 15), (':', 15), (')', 15), ('a', 15), ('first', 14)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bdaa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  637\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "def get_dict(data: list[str]) -> tuple:\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word\n",
    "\n",
    "\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8453bd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['&', \"'\", \"''\", \"'Bulletproof\", \"'s\", '(', ')', '.', '1.', '10']\n"
     ]
    }
   ],
   "source": [
    "print('bts' in word2Ind) \n",
    "print(list(word2Ind.keys())[:10])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0a3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(N: int, V: int, random_seed: int= 1) -> dict:\n",
    "    np.random.seed(random_seed)\n",
    "    parameters = {}\n",
    "\n",
    "    parameters[\"W1\"] = np.random.rand(N, V)\n",
    "    parameters[\"b1\"] = np.zeros(shape=(N, 1))\n",
    "\n",
    "    parameters[\"W2\"] = np.random.rand(V, N)\n",
    "    parameters[\"b2\"] = np.zeros(shape=(V, 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fcab548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (4, 10)\n",
      "W2 shape: (10, 4)\n",
      "b1 shape: (4, 1)\n",
      "b2 shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test he function example.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "params = init_parameters(tmp_N,tmp_V)\n",
    "assert params['W1'].shape == ((tmp_N,tmp_V))\n",
    "assert params['W2'].shape == ((tmp_V,tmp_N))\n",
    "print(f\"W1 shape: {params['W1'].shape}\")\n",
    "print(f\"W2 shape: {params['W2'].shape}\")\n",
    "print(f\"b1 shape: {params['b1'].shape}\")\n",
    "print(f\"b2 shape: {params['b2'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed7dd778",
   "metadata": {},
   "outputs": [],
   "source": [
    " # desine the activation function that are will be used in our NN later.\n",
    "from typing import List, Union\n",
    "\n",
    "def softmax(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
    "    y = np.exp(z) / np.sum(np.exp(z), 0, keepdims=True)\n",
    "\n",
    "    return y\n",
    "\n",
    "def sigmoid(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def relu(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9267b164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.73105858, 0.88079708],\n",
       "       [0.5       , 0.26894142, 0.11920292]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = np.array([[1,2,3],\n",
    "                [1,1,1]\n",
    "               ])\n",
    "tmp_sm = softmax(tmp)\n",
    "display(tmp_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68b47381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x: np.ndarray, params: dict) -> tuple:\n",
    "    z1 = np.dot(params['W1'], x) + params['b1']\n",
    "    h = relu(z1)\n",
    "\n",
    "    z2 = np.dot(params['W2'], h) + params['b2']\n",
    "    y_hat = z2\n",
    "\n",
    "    return y_hat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d19577fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call forward_prop\n",
      "\n",
      "z has shape (10, 1)\n",
      "z has values:\n",
      "[[1.82887324]\n",
      " [1.38466287]\n",
      " [0.60100484]\n",
      " [0.83284144]\n",
      " [1.37936774]\n",
      " [1.20420827]\n",
      " [1.26273995]\n",
      " [2.0149547 ]\n",
      " [1.10825153]\n",
      " [1.93910889]]\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "tmp_x = np.array([[0,1,0,0,0,0,0,0,0,0]]).T\n",
    "tmp_z, tmp_h = forward_propagation(tmp_x, params)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76485b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat: np.ndarray, y: np.ndarray,\n",
    "                       batch_size: int) -> Union[float, List[float]]:\n",
    "    logprobs = np.multiply(np.log(y_hat), y)\n",
    "    cost = -1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49dfa1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x: np.ndarray, y: np.ndarray, y_hat: np.ndarray, h: np.ndarray,\n",
    "                         params: dict, batch_size: int) -> dict:\n",
    "    grads_params = {}\n",
    "\n",
    "    l1 = np.dot(params['W2'].T ,(y_hat - y))\n",
    "\n",
    "    grads_params['W1'] = np.dot(l1, x.T) / batch_size\n",
    "    grads_params['W2'] = np.dot((y_hat - y), h.T) / batch_size\n",
    "\n",
    "    grads_params['b1'] = np.sum(l1, axis=1, keepdims= True) / batch_size\n",
    "    grads_params['b2'] = np.sum((y_hat - y), axis= 1, keepdims= True) / batch_size\n",
    "\n",
    "    return grads_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f82573b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_idx(words, word2Ind):\n",
    "    idx = []\n",
    "    for word in words:\n",
    "        idx = idx + [word2Ind[word]]\n",
    "    return idx\n",
    "\n",
    "def pack_idx_with_frequency(context_words, word2Ind):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for word in context_words:\n",
    "        freq_dict[word] += 1\n",
    "    idxs = get_idx(context_words, word2Ind)\n",
    "    packed = []\n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        freq = freq_dict[context_words[i]]\n",
    "        packed.append((idx, freq))\n",
    "    return packed\n",
    "\n",
    "def get_vectors(data, word2Ind, V, C):\n",
    "    i = C\n",
    "    while True:\n",
    "        y = np.zeros(V)\n",
    "        x = np.zeros(V)\n",
    "        center_word = data[i]\n",
    "        y[word2Ind[center_word]] = 1\n",
    "        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]\n",
    "        num_ctx_words = len(context_words)\n",
    "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
    "            x[idx] = freq / num_ctx_words\n",
    "        yield x, y\n",
    "        i += 1\n",
    "        if i >= len(data) - C:\n",
    "            print(\"i is being set to\", C)\n",
    "            i = C\n",
    "\n",
    "def get_batches(data, word2Ind, V, C, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for x, y in get_vectors(data, word2Ind, V, C):\n",
    "        if len(batch_x) < batch_size:\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        else:\n",
    "            yield np.array(batch_x).T, np.array(batch_y).T\n",
    "            batch_x = []\n",
    "            batch_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6415e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data: list[str], word2Ind: dict, N: int,\n",
    "                     V: int, epochs: int, alpha: float= 0.03,\n",
    "                     random_seed: int= 282) -> dict:\n",
    "\n",
    "    param = init_parameters(N,V, random_seed=random_seed)\n",
    "\n",
    "    batch_size = 128\n",
    "    epoch = 0\n",
    "    C = 2\n",
    "\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "\n",
    "        z, h = forward_propagation(x, param)\n",
    "        yhat = softmax(z)\n",
    "\n",
    "        cost = cross_entropy_loss(yhat, y, batch_size)\n",
    "        if ( (epoch+1) % 10 == 0):\n",
    "            print(f\"iters: {epoch + 1} cost: {cost:.6f}\")\n",
    "\n",
    "\n",
    "        grads = backward_propagation(x,y, yhat, h, param, batch_size)\n",
    "\n",
    "\n",
    "        param['W1'] = param['W1'] - alpha * grads['W1']\n",
    "        param['W2'] = param['W2'] - alpha * grads['W2']\n",
    "        param['b1'] = param['b1'] - alpha * grads['b1']\n",
    "        param['b2'] = param['b2'] - alpha * grads['b2']\n",
    "\n",
    "        epoch +=1\n",
    "        if epoch == epochs:\n",
    "            break\n",
    "        if epoch % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44227c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 6.827731\n",
      "i is being set to 2\n",
      "iters: 20 cost: 6.802057\n",
      "i is being set to 2\n",
      "iters: 30 cost: 6.639857\n",
      "i is being set to 2\n",
      "iters: 40 cost: 6.598190\n",
      "iters: 50 cost: 6.633337\n",
      "i is being set to 2\n",
      "iters: 60 cost: 6.499403\n",
      "i is being set to 2\n",
      "iters: 70 cost: 6.407509\n",
      "i is being set to 2\n",
      "iters: 80 cost: 6.284237\n",
      "iters: 90 cost: 6.338937\n",
      "i is being set to 2\n",
      "iters: 100 cost: 6.094671\n",
      "i is being set to 2\n",
      "iters: 110 cost: 6.238966\n",
      "i is being set to 2\n",
      "iters: 120 cost: 6.206588\n",
      "iters: 130 cost: 6.209357\n",
      "i is being set to 2\n",
      "iters: 140 cost: 5.965101\n",
      "i is being set to 2\n",
      "iters: 150 cost: 6.091380\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient_descent\")\n",
    "params = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54dd85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
